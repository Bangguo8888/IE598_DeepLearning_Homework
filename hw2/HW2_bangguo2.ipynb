{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning HW2\n",
    "### Name: Bangguo Wang            \n",
    "### NetID: bangguo2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: \n",
    "#### (1)\n",
    "In the forward propogation with dropout, from the layer $L_l$ to layer $L_{l+1}$, </br > \n",
    "$$Z_{l+1}=\\frac{W_{l+1}(A_{l}*D_{l})}{keep\\_prob}+B_{l+1}$$, where W and B are the parameters, A is the input matrix and D is the dropout matrix.\n",
    "so if we know the Derivative of $Z_{l+1}: dZ_{l+1}= \\frac{\\partial Loss}{\\partial Z_{l+1}},$ \n",
    "then we can develop, $$dA_{l}=\\frac{\\partial Z_{l+1}}{\\partial A_{l}} \\times dZ_{l+1} = \\frac{W_{l+1}^T*D_{l}}{keep\\_prob} \\times dZ_{l+1}$$\n",
    "and because in the forward propogation, $A_{l}=g(Z_{l})$, where g(*) is an activation function,\n",
    "so $Z_{l}=g^{-1}(A_{l})$,therefore, $$dZ_{l}=\\frac{\\partial g^{-1}(A_{l})}{\\partial Z_{l}}*dA_{l}$$\n",
    "Therefore, by the values of $dZ_{l}$,we can get the partical derivative of $W_l$ and $B_l$:\n",
    "$$dW_{l}=dZ_{l}A^{T}_{l-1}$$,$$dB_{l}=dZ_{l}$$  \n",
    "#### (2)\n",
    "State the stochastic gradient descent algorithm for this neural network:  \n",
    "1. Denote the batch size is m, so every time we train the model,we will randomly choose m samples from the train data set as the input\n",
    "2. Do the forward proprogation with dropout process and calculate the loss (the average of the m samples' loss)\n",
    "3. Do the backward propagation with dropout, and calculate the average of the m samples' derivative for the perspective of $W$ and $B$\n",
    "4. Update the parameters $W$ and $B$. \n",
    "5. Continue 1~4 steps as describe above,until it reaches the maxinum iterations. \n",
    "\n",
    "Specifically,when the m equals to 1, which means in every iteration, just randomly choose one sample to calculate the process described above, then this algorithm is stochastic gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the experiments, without dropout, the test accuracy is 0.9883. \n",
    "With dropout, the test accuracy is 0.9932. \n",
    "The codes are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"add_3:0\", shape=(?, 28, 28, 32), dtype=float32)\n",
      "Tensor(\"MaxPool_3:0\", shape=(?, 14, 14, 32), dtype=float32)\n",
      "0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6e3b1d765063>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    106\u001b[0m             \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             \u001b[0mY_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mY_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mY_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bangguo/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, feed_dict, session)\u001b[0m\n\u001b[1;32m   1586\u001b[0m         \u001b[0mnone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0msession\u001b[0m \u001b[0mwill\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1587\u001b[0m     \"\"\"\n\u001b[0;32m-> 1588\u001b[0;31m     \u001b[0m_run_using_default_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1589\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bangguo/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_run_using_default_session\u001b[0;34m(operation, feed_dict, graph, session)\u001b[0m\n\u001b[1;32m   3830\u001b[0m                        \u001b[0;34m\"the operation's graph is different from the session's \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3831\u001b[0m                        \"graph.\")\n\u001b[0;32m-> 3832\u001b[0;31m   \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3833\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bangguo/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bangguo/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bangguo/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/bangguo/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/bangguo/anaconda/envs/py3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import random\n",
    "\n",
    "# load MNIST data\n",
    "MNIST_data = h5py.File('MNISTdata.hdf5', 'r')\n",
    "x_train = np.float32(MNIST_data['x_train'][:])\n",
    "y_train = np.int32(np.array(MNIST_data['y_train'][:, 0]))\n",
    "x_test = np.float32(MNIST_data['x_test'][:])\n",
    "y_test = np.int32(np.array(MNIST_data['y_test'][:, 0]))\n",
    "MNIST_data.close()\n",
    "\n",
    "\n",
    "# organize the data\n",
    "X_train = x_train.reshape([-1,28,28,1])\n",
    "X_test = x_test.reshape([-1,28,28,1])\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "Y_train = encoder.fit_transform(y_train.reshape(-1,1)).todense()\n",
    "Y_test = encoder.transform(y_test.reshape(-1,1)).todense()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def initialize_W(shape):\n",
    "    initial = tf.truncated_normal(shape,stddev = np.sqrt(2./(shape[0]+shape[1])))\n",
    "    W = tf.Variable(initial)\n",
    "    return W\n",
    "def initialize_b(shape):\n",
    "    initial = tf.constant(0.0,shape = shape)\n",
    "    b = tf.Variable(initial)\n",
    "    return b\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32,[None,28,28,1])\n",
    "# conv layer1\n",
    "W1 = initialize_W([5,5,1,32])\n",
    "b1 = initialize_b([32])\n",
    "\n",
    "conv_1 = tf.nn.conv2d(X, W1, strides=[1, 1, 1, 1], padding='SAME') + b1\n",
    "print(conv_1)\n",
    "A_1 = tf.nn.relu(conv_1)\n",
    "pool_1 = tf.nn.max_pool(A_1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "print(pool_1)\n",
    "\n",
    "# conv layer2\n",
    "W2 = initialize_W([3,3,32,64])\n",
    "b2 = initialize_b([64])\n",
    "conv_2 = tf.nn.conv2d(pool_1, W2, strides=[1, 1, 1, 1], padding='SAME') + b2\n",
    "A_2 = tf.nn.relu(conv_2)\n",
    "pool_2 = tf.nn.max_pool(A_2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# conv layer3\n",
    "W3 = initialize_W([3,3,64,128])\n",
    "b3 = initialize_b([128])\n",
    "conv_3 = tf.nn.conv2d(pool_2, W3, strides=[1, 1, 1, 1], padding='SAME') + b3\n",
    "A_3 = tf.nn.relu(conv_3)\n",
    "pool_3 = tf.nn.max_pool(A_3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# dropout\n",
    "keep_prob = tf.placeholder(\"float\")\n",
    "D3 = tf.nn.dropout(pool_3, keep_prob)\n",
    "\n",
    "# full connected layer\n",
    "flat_layer = tf.reshape(pool_3, [-1,2048])\n",
    "W4 = tf.Variable(tf.truncated_normal([2048,50],stddev = np.sqrt(1./2048)))\n",
    "b4 = tf.Variable(tf.constant(0.0,shape = [50]))\n",
    "Z4 = tf.add(tf.matmul(flat_layer,W4),b4)\n",
    "A4 = tf.nn.relu(Z4)\n",
    "\n",
    "# dropout\n",
    "D4 = tf.nn.dropout(A4, keep_prob)\n",
    "\n",
    "# softmax layer\n",
    "W5 = tf.Variable(tf.truncated_normal([50,10],stddev = np.sqrt(1./50)))\n",
    "b5 = tf.Variable(tf.constant(0.0,shape = [10]))\n",
    "Z5 = tf.add(tf.matmul(D4,W5),b5)\n",
    "A5 = tf.nn.softmax(Z5)\n",
    "\n",
    "# loss\n",
    "Y = tf.placeholder(tf.float32,[None,10])\n",
    "loss = -tf.reduce_mean(Y*tf.log(tf.clip_by_value(A5,1e-11,1.0)))\n",
    "\n",
    "# train\n",
    "train_step = tf.train.AdamOptimizer(1e-3).minimize(loss)\n",
    "\n",
    "y_pred = tf.arg_max(A5,1)\n",
    "bool_pred = tf.equal(tf.arg_max(Y,1),y_pred)\n",
    "accuracy = tf.reduce_mean(tf.cast(bool_pred,tf.float32))\n",
    "\n",
    "\n",
    "# session\n",
    "batch_size = 32\n",
    "sample_size = np.shape(x_train)[0]\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    epoch = 15\n",
    "    for i in range(epoch):\n",
    "        index = np.arange(sample_size)\n",
    "        random.shuffle(index)\n",
    "        for j in range(sample_size // batch_size):\n",
    "            start = j * batch_size\n",
    "            end = min((j + 1) * batch_size, sample_size)\n",
    "            X_batch = X_train[index[start:end]]\n",
    "            Y_batch = Y_train[index[start:end]]\n",
    "            train_step.run(feed_dict = {X: X_batch, Y: Y_batch,keep_prob:0.75})\n",
    "            if (j%100 == 0):\n",
    "                print(j)\n",
    "        if i % 1 == 0:\n",
    "            train_accuracy = accuracy.eval(feed_dict={X: X_batch, Y: Y_batch, keep_prob:1.0})\n",
    "            print(\"Epoch %d, Train accuracy is %g\"%(i+1, train_accuracy))\n",
    "\n",
    "    test_accuracy = accuracy.eval(feed_dict={X: X_test, Y: Y_test,keep_prob:1.0})\n",
    "    print(\"Test accuracy is %g\" % (test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test accuracy for different layers and different keep_probability, without data augmentation: \n",
    "\n",
    "|  | layers = 6 | layers = 10 | layers = 15 \n",
    "| :------:| :------: | :------: | :------: |\n",
    "| keep_prob=0.4 | 0.732812 | 0.757812 | 0.760312\n",
    "| keep_prob=0.7 | 0.742812 | 0.735313 | 0.737187 \n",
    "\n",
    "\n",
    "The test accuracy for different layers and different keep_probability, with data augmentation: \n",
    "\n",
    "|  | layers = 6 | layers = 10 | layers = 15 \n",
    "| :------:| :------: | :------: | :------: |\n",
    "| keep_prob=0.4 | 0.811875 | 0.885625 | 0.892188\n",
    "| keep_prob=0.7 | 0.834688 | 0.865937 | 0.849687   \n",
    " \n",
    "  \n",
    "One of the codes for the mode.py is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "def conv(x, w, b, stride, name):\n",
    "    with tf.variable_scope('conv'):\n",
    "        return tf.nn.conv2d(x,\n",
    "                            filter=w,\n",
    "                            strides=[1, stride, stride, 1],\n",
    "                            padding='SAME',\n",
    "                            name=name) + b\n",
    "\n",
    "\n",
    "######## after 30k iterations (batch_size=64)\n",
    "# with data augmentation (flip, brightness, contrast) ~81.0%\n",
    "# without data augmentation 69.6%\n",
    "def cifar10_conv(X, keep_prob, reuse=False):\n",
    "    with tf.variable_scope('cifar10_conv'):\n",
    "        if reuse:\n",
    "            tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        batch_size = tf.shape(X)[0]\n",
    "        K1 = 32\n",
    "        K2 = 32\n",
    "        K3 = 32\n",
    "        K4 = 48\n",
    "        K5 = 48\n",
    "        K6 = 80\n",
    "        K7 = 80\n",
    "        K8 = 80\n",
    "        K9 = 80\n",
    "        K10 = 80\n",
    "        K11 = 128\n",
    "        K12 = 128\n",
    "        K13 = 128\n",
    "        K14 = 128\n",
    "        K15 = 128\n",
    "        T = 73728\n",
    "        K16 = 500\n",
    "        \n",
    "        W1 = tf.get_variable('D_W1', [3, 3, 3, K1], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        print(W1)\n",
    "        B1 = tf.get_variable('D_B1', [K1], initializer=tf.constant_initializer())\n",
    "        conv1 = conv(X, W1, B1, stride=1, name='conv1')\n",
    "        bn1 = tf.nn.relu(tf.contrib.layers.batch_norm(conv1))\n",
    "\n",
    "        W2 = tf.get_variable('D_W2', [3, 3, K1, K2], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B2 = tf.get_variable('D_B2', [K2], initializer=tf.constant_initializer())\n",
    "        conv2 = conv(bn1, W2, B2, stride=1, name='conv2')\n",
    "        bn2 = tf.nn.relu(tf.contrib.layers.batch_norm(conv2))\n",
    "\n",
    "        W3 = tf.get_variable('D_W3', [3, 3, K2, K3], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B3 = tf.get_variable('D_B3', [K3], initializer=tf.constant_initializer())\n",
    "        conv3 = conv(bn2, W3, B3, stride=1, name='conv3')\n",
    "        bn3 = tf.nn.relu(tf.contrib.layers.batch_norm(conv3))\n",
    "        \n",
    "        W4 = tf.get_variable('D_W4', [3, 3, K3, K4], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B4 = tf.get_variable('D_B4', [K4], initializer=tf.constant_initializer())\n",
    "        conv4 = conv(bn3, W4, B4, stride=1, name='conv4')\n",
    "        bn4 = tf.nn.relu(tf.contrib.layers.batch_norm(conv4))\n",
    "        \n",
    "        W5 = tf.get_variable('D_W5', [3, 3, K4, K5], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B5 = tf.get_variable('D_B5', [K5], initializer=tf.constant_initializer())\n",
    "        conv5 = conv(bn4, W5, B5, stride=1, name='conv5')\n",
    "        bn5 = tf.nn.relu(tf.contrib.layers.batch_norm(conv5))\n",
    "        pooled5 = tf.nn.max_pool(bn5, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        d5 = tf.nn.dropout(pooled5, keep_prob)\n",
    "        \n",
    "        W6 = tf.get_variable('D_W6', [3, 3, K5, K6], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B6 = tf.get_variable('D_B6', [K6], initializer=tf.constant_initializer())\n",
    "        conv6 = conv(d5, W6, B6, stride=1, name='conv6')\n",
    "        bn6 = tf.nn.relu(tf.contrib.layers.batch_norm(conv6))\n",
    "        \n",
    "        W7 = tf.get_variable('D_W7', [3, 3, K6, K7], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B7 = tf.get_variable('D_B7', [K7], initializer=tf.constant_initializer())\n",
    "        conv7 = conv(bn6, W7, B7, stride=1, name='conv7')\n",
    "        bn7 = tf.nn.relu(tf.contrib.layers.batch_norm(conv7))\n",
    "        \n",
    "        W8 = tf.get_variable('D_W8', [3, 3, K7, K8], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B8 = tf.get_variable('D_B8', [K8], initializer=tf.constant_initializer())\n",
    "        conv8 = conv(bn7, W8, B8, stride=1, name='conv8')\n",
    "        bn8 = tf.nn.relu(tf.contrib.layers.batch_norm(conv8))\n",
    "        \n",
    "        W9 = tf.get_variable('D_W9', [3, 3, K8, K9], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B9 = tf.get_variable('D_B9', [K9], initializer=tf.constant_initializer())\n",
    "        conv9 = conv(bn8, W9, B9, stride=1, name='conv9')\n",
    "        bn9 = tf.nn.relu(tf.contrib.layers.batch_norm(conv9))\n",
    "        \n",
    "        W10 = tf.get_variable('D_W10', [3, 3, K9, K10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B10 = tf.get_variable('D_B10', [K10], initializer=tf.constant_initializer())\n",
    "        conv10 = conv(bn9, W10, B10, stride=1, name='conv10')\n",
    "        bn10 = tf.nn.relu(tf.contrib.layers.batch_norm(conv10))\n",
    "        pooled10 = tf.nn.max_pool(bn10, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        d10 = tf.nn.dropout(pooled10, keep_prob)\n",
    "        \n",
    "        \n",
    "        W11 = tf.get_variable('D_W11', [3, 3, K10, K11], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B11 = tf.get_variable('D_B11', [K11], initializer=tf.constant_initializer())\n",
    "        conv11 = conv(d10, W11, B11, stride=1, name='conv11')\n",
    "        bn11 = tf.nn.relu(tf.contrib.layers.batch_norm(conv11))\n",
    "        \n",
    "        W12 = tf.get_variable('D_W12', [3, 3, K11, K12], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B12 = tf.get_variable('D_B12', [K12], initializer=tf.constant_initializer())\n",
    "        conv12 = conv(bn11, W12, B12, stride=1, name='conv12')\n",
    "        bn12 = tf.nn.relu(tf.contrib.layers.batch_norm(conv12))\n",
    "        \n",
    "        W13 = tf.get_variable('D_W13', [3, 3, K12, K13], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B13 = tf.get_variable('D_B13', [K13], initializer=tf.constant_initializer())\n",
    "        conv13 = conv(bn12, W13, B13, stride=1, name='conv13')\n",
    "        bn13 = tf.nn.relu(tf.contrib.layers.batch_norm(conv13))\n",
    "        \n",
    "        W14 = tf.get_variable('D_W14', [3, 3, K13, K14], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B14 = tf.get_variable('D_B14', [K14], initializer=tf.constant_initializer())\n",
    "        conv14 = conv(bn13, W14, B14, stride=1, name='conv14')\n",
    "        bn14 = tf.nn.relu(tf.contrib.layers.batch_norm(conv14))\n",
    "        \n",
    "        W15 = tf.get_variable('D_W15', [3, 3, K14, K15], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B15 = tf.get_variable('D_B15', [K15], initializer=tf.constant_initializer())\n",
    "        conv15 = conv(bn14, W15, B15, stride=1, name='conv15')\n",
    "        bn15 = tf.nn.relu(tf.contrib.layers.batch_norm(conv15))\n",
    "        pooled15 = tf.nn.max_pool(bn15, ksize=[1, 2, 2, 1], strides=[1, 1, 1, 1], padding='SAME')\n",
    "        d15 = tf.nn.dropout(pooled15, keep_prob)\n",
    "        \n",
    "        \n",
    "        flat = tf.reshape(d15, [batch_size, T])\n",
    "    \n",
    "        W16 = tf.get_variable('D_W16', [T, K16], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B16 = tf.get_variable('D_B16', [K16], initializer=tf.constant_initializer())\n",
    "        M16 = tf.matmul(flat, W16) + B16\n",
    "        bn16 = tf.nn.relu(tf.contrib.layers.batch_norm(M16))\n",
    "        d16 = tf.nn.dropout(bn16, keep_prob)\n",
    "        \n",
    "        \n",
    "        W17 = tf.get_variable('D_W17', [K16, 10], initializer=tf.contrib.layers.xavier_initializer())\n",
    "        B17 = tf.get_variable('D_B17', [10], initializer=tf.constant_initializer())\n",
    "        M17 = tf.matmul(d16, W17) + B17\n",
    "        output = tf.nn.softmax(M17)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3kernel",
   "language": "python",
   "name": "py3kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
